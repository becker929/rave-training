{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c82e8e56-c606-4ea5-8992-7dfb27049403",
   "metadata": {},
   "source": [
    "# TODO\n",
    "- get rid of the gin config stuff\n",
    "    - replace everything in v1 with variables & reference those vars\n",
    "    - same for v2\n",
    "- make copy & reduce to entry point\n",
    "- compile entry point\n",
    "- re-add next section of graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d1dfcb-9f8c-41de-aa73-4e757bf44746",
   "metadata": {
    "tags": []
   },
   "source": [
    "## directory & imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6c804d8-3ead-432d-a3f5-cc6e98bb63dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/rave-compile\n"
     ]
    }
   ],
   "source": [
    "%cd /home/ubuntu/rave-compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d05d7fbf-bf31-4fc8-8c3c-fc6528a17fc2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import gin\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import pathlib\n",
    "import cached_conv as cc\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "import cached_conv as cc\n",
    "import gin\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import weight_norm\n",
    "from torchaudio.transforms import Spectrogram\n",
    "\n",
    "from rave.core import amp_to_impulse_response, fft_convolve, mod_sigmoid\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from random import random\n",
    "from typing import Callable, Optional, Sequence, Union\n",
    "\n",
    "import GPUtil as gpu\n",
    "import librosa as li\n",
    "import lmdb\n",
    "import numpy as np\n",
    "import torch.fft as fft\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "from einops import rearrange\n",
    "from scipy.signal import lfilter\n",
    "\n",
    "import base64\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import subprocess\n",
    "from random import random\n",
    "from typing import Dict, Iterable, Optional, Sequence, Callable, Tuple, Type, Any, Union\n",
    "\n",
    "import numpy as np\n",
    "import requests\n",
    "import yaml\n",
    "from scipy.signal import lfilter\n",
    "from torch.utils import data\n",
    "from tqdm import tqdm\n",
    "from udls import AudioExample as AudioExampleWrapper\n",
    "from udls import transforms\n",
    "from udls.generated import AudioExample\n",
    "\n",
    "from time import time\n",
    "\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9440be5f-f73e-4921-b28b-795ac113aa88",
   "metadata": {
    "tags": []
   },
   "source": [
    "## config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "142d46a3-ca21-4c34-a346-cb3eedc70a52",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3050905911.py, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[17], line 9\u001b[0;36m\u001b[0m\n\u001b[0;31m    core.AudioDistanceV1:\u001b[0m\n\u001b[0m                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "SAMPLING_RATE = 44100\n",
    "CAPACITY = 64\n",
    "N_BAND = 16\n",
    "LATENT_SIZE = 128\n",
    "RATIOS = [4, 4, 4, 2]\n",
    "PHASE_1_DURATION = 1000000\n",
    "\n",
    "# CORE CONFIGURATION\n",
    "core.AudioDistanceV1:\n",
    "    multiscale_stft = @core.MultiScaleSTFT\n",
    "    log_epsilon = 1e-7\n",
    "\n",
    "core.MultiScaleSTFT:\n",
    "    scales = [2048, 1024, 512, 256, 128]\n",
    "    sample_rate = %SAMPLING_RATE\n",
    "    magnitude = True\n",
    "\n",
    "dataset.split_dataset.max_residual = 1000\n",
    "\n",
    "# CONVOLUTION CONFIGURATION\n",
    "cc.Conv1d.bias = False\n",
    "cc.ConvTranspose1d.bias = False\n",
    "\n",
    "# PQMF\n",
    "pqmf.CachedPQMF:\n",
    "    attenuation = 100\n",
    "    n_band = %N_BAND\n",
    "\n",
    "blocks.normalization.mode = 'weight_norm'\n",
    "\n",
    "# ENCODER\n",
    "blocks.Encoder:\n",
    "    data_size = %N_BAND\n",
    "    capacity = %CAPACITY\n",
    "    latent_size = %LATENT_SIZE\n",
    "    ratios = %RATIOS\n",
    "    sample_norm = False\n",
    "    repeat_layers = 1\n",
    "\n",
    "variational/blocks.Encoder.n_out = 2\n",
    "\n",
    "blocks.VariationalEncoder:\n",
    "    encoder = @variational/blocks.Encoder\n",
    "\n",
    "# DECODER\n",
    "blocks.Generator:\n",
    "    latent_size = %LATENT_SIZE\n",
    "    capacity = %CAPACITY\n",
    "    data_size = %N_BAND\n",
    "    ratios = %RATIOS\n",
    "    loud_stride = 1\n",
    "    use_noise = True\n",
    "\n",
    "blocks.ResidualStack:\n",
    "    kernel_sizes = [3]\n",
    "    dilations_list = [[1, 1], [3, 1], [5, 1]]\n",
    "\n",
    "blocks.NoiseGenerator:\n",
    "    ratios = [4, 4, 4]\n",
    "    noise_bands = 5\n",
    "\n",
    "# DISCRIMINATOR\n",
    "discriminator.ConvNet:\n",
    "    in_size = 1\n",
    "    out_size = 1\n",
    "    capacity = %CAPACITY\n",
    "    n_layers = 4\n",
    "    stride = 4\n",
    "\n",
    "scales/discriminator.ConvNet:\n",
    "    conv = @torch.nn.Conv1d\n",
    "    kernel_size = 15\n",
    "\n",
    "discriminator.MultiScaleDiscriminator:\n",
    "    n_discriminators = 3\n",
    "    convnet = @scales/discriminator.ConvNet\n",
    "\n",
    "feature_matching/core.mean_difference:\n",
    "    norm = 'L1'\n",
    "\n",
    "# BALANCER\n",
    "balancer.Balancer:\n",
    "    ema_averager = @balancer.EMA\n",
    "    weights = {\n",
    "        'regularization': .1,\n",
    "        'feature_matching': 10,\n",
    "    }\n",
    "    deny_list = [\n",
    "        'regularization'\n",
    "    ]\n",
    "    scale_gradients = False\n",
    "\n",
    "balancer.EMA:\n",
    "    beta = 0.999\n",
    "\n",
    "# MODEL ASSEMBLING\n",
    "latent_size = %LATENT_SIZE\n",
    "pqmf = @pqmf.CachedPQMF\n",
    "sampling_rate = %SAMPLING_RATE\n",
    "encoder = @blocks.VariationalEncoder  \n",
    "decoder = @blocks.Generator\n",
    "discriminator = @discriminator.MultiScaleDiscriminator\n",
    "phase_1_duration = %PHASE_1_DURATION\n",
    "gan_loss = @core.hinge_gan\n",
    "valid_signal_crop = False\n",
    "feature_matching_fun = @feature_matching/core.mean_difference\n",
    "num_skipped_features = 0\n",
    "audio_distance = @core.AudioDistanceV1\n",
    "multiband_audio_distance = @core.AudioDistanceV1\n",
    "balancer = @balancer.Balancer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea4ae59-bd32-4b5b-a935-55eacb75396f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "390f3674-84c5-408e-ba53-ecce92bd14b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EMA:\n",
    "    def __init__(self, beta: float = 0.999) -> None:\n",
    "        self.shadows = {}\n",
    "        self.beta = beta\n",
    "\n",
    "    def __call__(self, inputs: Dict[str, torch.Tensor]):\n",
    "        outputs = {}\n",
    "        for k, v in inputs.items():\n",
    "            if not k in self.shadows:\n",
    "                self.shadows[k] = v.to(\"cuda\")\n",
    "            else:\n",
    "                self.shadows[k] *= self.beta\n",
    "                self.shadows[k] += (1 - self.beta) * v\n",
    "\n",
    "            outputs[k] = self.shadows[k].clone()\n",
    "        return outputs\n",
    "    \n",
    "    \n",
    "class Balancer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        ema_averager: Callable[[], EMA],\n",
    "        weights: Dict[str, float],\n",
    "        scale_gradients: bool = False,\n",
    "        deny_list: Optional[Sequence[str]] = None,\n",
    "    ) -> None:\n",
    "        self.ema_averager = ema_averager()\n",
    "        self.weights = weights\n",
    "        self.scale_gradients = scale_gradients\n",
    "        self.deny_list = deny_list\n",
    "\n",
    "    def backward(\n",
    "        self,\n",
    "        losses: Dict[str, torch.Tensor],\n",
    "        model_output: torch.Tensor,\n",
    "        logger: Optional[Callable[[str, float], None]] = None,\n",
    "        profiler: Optional[Any] = None,\n",
    "    ):\n",
    "        grads = {}\n",
    "        norms = {}\n",
    "\n",
    "        for k, v in losses.items():\n",
    "            if self.deny_list is not None:\n",
    "                if k in self.deny_list:\n",
    "                    continue\n",
    "\n",
    "            (grads[k],) = torch.autograd.grad(\n",
    "                v.to(\"cuda\"),\n",
    "                [model_output.to(\"cuda\")],\n",
    "                retain_graph=True,\n",
    "            )\n",
    "\n",
    "            if (nans := torch.isnan(grads[k].to('cuda'))).any():\n",
    "                count = nans.float().to(\"cuda\").mean()\n",
    "                grads[k] = torch.where(\n",
    "                    nans, torch.zeros_like(nans, device=\"cuda\"), grads[k]\n",
    "                )\n",
    "                if logger is not None:\n",
    "                    logger(f\"{k}_nan_ratio\", count)\n",
    "\n",
    "            norms[k] = grads[k].to(\"cuda\").norm(dim=tuple(range(1, grads[k].dim()))).mean()\n",
    "\n",
    "            if profiler is not None:\n",
    "                profiler(f\"partial backward {k}\")\n",
    "\n",
    "        avg_norms = self.ema_averager(norms)\n",
    "\n",
    "        if profiler is not None:\n",
    "            profiler(\"grad norm estimation\")\n",
    "\n",
    "        sum_weights = sum([self.weights.get(k, 1) for k in avg_norms])\n",
    "\n",
    "        for name, norm in avg_norms.items():\n",
    "            if self.scale_gradients:\n",
    "                ratio = self.weights.get(name, 1) / sum_weights\n",
    "                scale = ratio / (norm + 1e-6)\n",
    "\n",
    "                if logger is not None:\n",
    "                    logger(f\"scale_{name}\", scale)\n",
    "                    logger(f\"grad_norm_{name}\", grads[name].norm())\n",
    "                    logger(f\"target_norm_{name}\", ratio)\n",
    "            else:\n",
    "                scale = self.weights.get(name, 1)\n",
    "\n",
    "                if logger is not None:\n",
    "                    logger(f\"scale_{name}\", scale)\n",
    "                    logger(f\"grad_norm_{name}\", grads[name].norm())\n",
    "\n",
    "        if profiler is not None:\n",
    "            profiler(\"norm scaling\")\n",
    "\n",
    "        full_grad = sum([grads[name].to(\"cuda\") for name in avg_norms.keys()]).to(\"cuda\")\n",
    "        model_output.backward(full_grad, retain_graph=True)\n",
    "\n",
    "        if profiler is not None:\n",
    "            profiler(\"scaled backward\")\n",
    "\n",
    "        if self.deny_list is not None:\n",
    "            for k in self.deny_list:\n",
    "                if k in losses:\n",
    "                    loss = losses[k].to(\"cuda\") * self.weights.get(k, 1)\n",
    "                    if logger is not None:\n",
    "                        logger(f\"scale_{name}\", scale)\n",
    "                        logger(f\"grad_norm_{name}\", grads[name].norm())\n",
    "                    if loss.requires_grad:\n",
    "                        loss.backward(retain_graph=True)\n",
    "\n",
    "        if profiler is not None:\n",
    "            profiler(\"denied backward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94674b23-d874-40f5-88df-0a45e6898fa1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "__file__ = \"/home/ubuntu/rave-training/rave/__init__.py\"\n",
    "\n",
    "gin.enter_interactive_mode()\n",
    "gin.add_config_file_search_path(os.path.dirname(__file__))\n",
    "gin.add_config_file_search_path(\n",
    "    os.path.join(\n",
    "        os.path.dirname(__file__),\n",
    "        'configs',\n",
    "    ))\n",
    "\n",
    "cc.get_padding = gin.external_configurable(cc.get_padding, module=\"cc\")\n",
    "cc.Conv1d = gin.external_configurable(cc.Conv1d, module=\"cc\")\n",
    "cc.ConvTranspose1d = gin.external_configurable(cc.ConvTranspose1d, module=\"cc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ade86c53-5576-44b7-9cbe-cb0691fd18e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def spectrogram(n_fft: int):\n",
    "    return torchaudio.transforms.Spectrogram(\n",
    "        n_fft,\n",
    "        hop_length=n_fft // 4,\n",
    "        power=None,\n",
    "        normalized=True,\n",
    "        center=False,\n",
    "        pad_mode=None,\n",
    "    )\n",
    "\n",
    "\n",
    "def rectified_2d_conv_block(\n",
    "    capacity,\n",
    "    kernel_sizes,\n",
    "    strides: Optional[Tuple[int, int]] = None,\n",
    "    dilations: Optional[Tuple[int, int]] = None,\n",
    "    in_size: Optional[int] = None,\n",
    "    out_size: Optional[int] = None,\n",
    "    activation: bool = True,\n",
    "):\n",
    "    if dilations is None:\n",
    "        paddings = kernel_sizes[0] // 2, kernel_sizes[1] // 2\n",
    "    else:\n",
    "        fks = (kernel_sizes[0] - 1) * dilations[0], (kernel_sizes[1] -\n",
    "                                                     1) * dilations[1]\n",
    "        paddings = fks[0] // 2, fks[1] // 2\n",
    "\n",
    "    conv = normalization(\n",
    "        nn.Conv2d(\n",
    "            in_size or capacity,\n",
    "            out_size or capacity,\n",
    "            kernel_size=kernel_sizes,\n",
    "            stride=strides or (1, 1),\n",
    "            dilation=dilations or (1, 1),\n",
    "            padding=paddings,\n",
    "        ))\n",
    "\n",
    "    if not activation: return conv\n",
    "\n",
    "    return nn.Sequential(conv, nn.LeakyReLU(.2))\n",
    "\n",
    "\n",
    "class EncodecConvNet(nn.Module):\n",
    "\n",
    "    def __init__(self, capacity: int) -> None:\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            rectified_2d_conv_block(capacity, (9, 3), in_size=2),\n",
    "            rectified_2d_conv_block(capacity, (9, 3), (2, 1), (1, 1)),\n",
    "            rectified_2d_conv_block(capacity, (9, 3), (2, 1), (1, 2)),\n",
    "            rectified_2d_conv_block(capacity, (9, 3), (2, 1), (1, 4)),\n",
    "            rectified_2d_conv_block(capacity, (3, 3)),\n",
    "            rectified_2d_conv_block(capacity, (3, 3),\n",
    "                                    out_size=1,\n",
    "                                    activation=False),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = []\n",
    "        for layer in self.net:\n",
    "            x = layer(x)\n",
    "            features.append(x)\n",
    "        return features\n",
    "\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "\n",
    "    def __init__(self, in_size, out_size, capacity, n_layers, kernel_size,\n",
    "                 stride, conv) -> None:\n",
    "        super().__init__()\n",
    "        channels = [in_size]\n",
    "        channels += list(capacity * 2**np.arange(n_layers))\n",
    "\n",
    "        if isinstance(stride, int):\n",
    "            stride = n_layers * [stride]\n",
    "\n",
    "        net = []\n",
    "        for i in range(n_layers):\n",
    "            if not isinstance(kernel_size, int):\n",
    "                pad = (cc.get_padding(kernel_size[0],\n",
    "                                      stride[i],\n",
    "                                      mode=\"centered\")[0], 0)\n",
    "                s = (stride[i], 1)\n",
    "            else:\n",
    "                pad = cc.get_padding(kernel_size, stride[i],\n",
    "                                     mode=\"centered\")[0]\n",
    "                s = stride[i]\n",
    "            net.append(\n",
    "                normalization(\n",
    "                    conv(\n",
    "                        channels[i],\n",
    "                        channels[i + 1],\n",
    "                        kernel_size,\n",
    "                        stride=s,\n",
    "                        padding=pad,\n",
    "                    )))\n",
    "            net.append(nn.LeakyReLU(.2))\n",
    "        net.append(conv(channels[-1], out_size, 1))\n",
    "\n",
    "        self.net = nn.Sequential(*net)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = []\n",
    "        for layer in self.net:\n",
    "            x = layer(x)\n",
    "            if isinstance(layer, nn.modules.conv._ConvNd):\n",
    "                features.append(x)\n",
    "        return features\n",
    "\n",
    "\n",
    "class MultiScaleDiscriminator(nn.Module):\n",
    "\n",
    "    def __init__(self, n_discriminators, convnet) -> None:\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for i in range(n_discriminators):\n",
    "            layers.append(convnet())\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = []\n",
    "        for layer in self.layers:\n",
    "            features.append(layer(x))\n",
    "            x = nn.functional.avg_pool1d(x, 2)\n",
    "        return features\n",
    "\n",
    "\n",
    "class MultiScaleSpectralDiscriminator(nn.Module):\n",
    "\n",
    "    def __init__(self, scales: Sequence[int],\n",
    "                 convnet: Callable[[], nn.Module]) -> None:\n",
    "        super().__init__()\n",
    "        self.specs = nn.ModuleList([spectrogram(n) for n in scales])\n",
    "        self.nets = nn.ModuleList([convnet() for _ in scales])\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = []\n",
    "        for spec, net in zip(self.specs, self.nets):\n",
    "            spec_x = spec(x)\n",
    "            spec_x = torch.cat([spec_x.real, spec_x.imag], 1)\n",
    "            features.append(net(spec_x))\n",
    "        return features\n",
    "\n",
    "\n",
    "class MultiScaleSpectralDiscriminator1d(nn.Module):\n",
    "\n",
    "    def __init__(self, scales: Sequence[int],\n",
    "                 convnet: Callable[[int], nn.Module]) -> None:\n",
    "        super().__init__()\n",
    "        self.specs = nn.ModuleList([spectrogram(n) for n in scales])\n",
    "        self.nets = nn.ModuleList([convnet(n + 2) for n in scales])\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = []\n",
    "        for spec, net in zip(self.specs, self.nets):\n",
    "            spec_x = spec(x).squeeze(1)\n",
    "            spec_x = torch.cat([spec_x.real, spec_x.imag], 1)\n",
    "            features.append(net(spec_x))\n",
    "        return features\n",
    "\n",
    "\n",
    "class MultiPeriodDiscriminator(nn.Module):\n",
    "\n",
    "    def __init__(self, periods, convnet) -> None:\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        self.periods = periods\n",
    "\n",
    "        for _ in periods:\n",
    "            layers.append(convnet())\n",
    "\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = []\n",
    "        for layer, n in zip(self.layers, self.periods):\n",
    "            features.append(layer(self.fold(x, n)))\n",
    "        return features\n",
    "\n",
    "    def fold(self, x, n):\n",
    "        pad = (n - (x.shape[-1] % n)) % n\n",
    "        x = nn.functional.pad(x, (0, pad))\n",
    "        return x.reshape(*x.shape[:2], -1, n)\n",
    "\n",
    "\n",
    "class CombineDiscriminators(nn.Module):\n",
    "\n",
    "    def __init__(self, discriminators: Sequence[Type[nn.Module]]) -> None:\n",
    "        super().__init__()\n",
    "        self.discriminators = nn.ModuleList(disc_cls()\n",
    "                                            for disc_cls in discriminators)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = []\n",
    "        for disc in self.discriminators:\n",
    "            features.extend(disc(x))\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0e83312-1fca-48c5-b2af-7634b8cab273",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Profiler:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.ticks = [[time(), None]]\n",
    "\n",
    "    def tick(self, msg):\n",
    "        self.ticks.append([time(), msg])\n",
    "\n",
    "    def __repr__(self):\n",
    "        rep = 80 * \"=\" + \"\\n\"\n",
    "        for i in range(1, len(self.ticks)):\n",
    "            msg = self.ticks[i][1]\n",
    "            ellapsed = self.ticks[i][0] - self.ticks[i - 1][0]\n",
    "            rep += msg + f\": {ellapsed*1000:.2f}ms\\n\"\n",
    "        rep += 80 * \"=\" + \"\\n\\n\\n\"\n",
    "        return rep\n",
    "\n",
    "\n",
    "class WarmupCallback(pl.Callback):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.state = {'training_steps': 0}\n",
    "\n",
    "    def on_train_batch_start(self, trainer, pl_module, batch,\n",
    "                             batch_idx) -> None:\n",
    "        if self.state['training_steps'] >= pl_module.warmup:\n",
    "            pl_module.warmed_up = True\n",
    "        self.state['training_steps'] += 1\n",
    "\n",
    "    def state_dict(self):\n",
    "        return self.state.copy()\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        self.state.update(state_dict)\n",
    "\n",
    "\n",
    "class QuantizeCallback(WarmupCallback):\n",
    "\n",
    "    def on_train_batch_start(self, trainer, pl_module, batch,\n",
    "                             batch_idx) -> None:\n",
    "\n",
    "        if pl_module.warmup_quantize is None: return\n",
    "\n",
    "        if self.state['training_steps'] >= pl_module.warmup_quantize:\n",
    "            if isinstance(pl_module.encoder, DiscreteEncoder):\n",
    "                pl_module.encoder.enabled = torch.tensor(1, device=\"cuda\").type_as(\n",
    "                    pl_module.encoder.enabled)\n",
    "        self.state['training_steps'] += 1\n",
    "\n",
    "\n",
    "@gin.configurable\n",
    "class RAVE(pl.LightningModule):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        latent_size,\n",
    "        sampling_rate,\n",
    "        encoder,\n",
    "        decoder,\n",
    "        discriminator,\n",
    "        phase_1_duration,\n",
    "        gan_loss,\n",
    "        valid_signal_crop,\n",
    "        feature_matching_fun,\n",
    "        num_skipped_features,\n",
    "        audio_distance: Callable[[], nn.Module],\n",
    "        multiband_audio_distance: Callable[[], nn.Module],\n",
    "        balancer: Callable[[], Balancer],\n",
    "        warmup_quantize: Optional[int] = None,\n",
    "        pqmf: Optional[Callable[[], nn.Module]] = None,\n",
    "        update_discriminator_every: int = 2,\n",
    "        enable_pqmf_encode: bool = True,\n",
    "        enable_pqmf_decode: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pqmf = None\n",
    "        if pqmf is not None:\n",
    "            self.pqmf = pqmf().to(\"cuda\")\n",
    "\n",
    "        self.encoder = encoder().to(\"cuda\")\n",
    "        self.decoder = decoder().to(\"cuda\")\n",
    "        self.discriminator = discriminator().to(\"cuda\")\n",
    "\n",
    "        self.audio_distance = audio_distance().to(\"cuda\")\n",
    "        self.multiband_audio_distance = multiband_audio_distance().to(\"cuda\")\n",
    "\n",
    "        self.gan_loss = gan_loss\n",
    "\n",
    "        self.register_buffer(\"latent_pca\", torch.eye(latent_size, device=\"cuda\"))\n",
    "        self.register_buffer(\"latent_mean\", torch.zeros(latent_size, device=\"cuda\"))\n",
    "        self.register_buffer(\"fidelity\", torch.zeros(latent_size, device=\"cuda\"))\n",
    "\n",
    "        self.latent_size = latent_size\n",
    "\n",
    "        self.automatic_optimization = False\n",
    "\n",
    "        # SCHEDULE\n",
    "        self.warmup = 500 # phase_1_duration\n",
    "        self.warmup_quantize = warmup_quantize\n",
    "        self.balancer = balancer()\n",
    "\n",
    "        self.warmed_up = False\n",
    "\n",
    "        # CONSTANTS\n",
    "        self.sr = sampling_rate\n",
    "        self.valid_signal_crop = valid_signal_crop\n",
    "        self.feature_matching_fun = feature_matching_fun\n",
    "        self.num_skipped_features = num_skipped_features\n",
    "        self.update_discriminator_every = update_discriminator_every\n",
    "\n",
    "        self.eval_number = 0\n",
    "        self.integrator = None\n",
    "\n",
    "        self.enable_pqmf_encode = enable_pqmf_encode\n",
    "        self.enable_pqmf_decode = enable_pqmf_decode\n",
    "\n",
    "        self.register_buffer(\"receptive_field\", torch.tensor([0, 0], device=\"cuda\").long())\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        gen_p = list(self.encoder.parameters())\n",
    "        gen_p += list(self.decoder.parameters())\n",
    "        dis_p = list(self.discriminator.parameters())\n",
    "\n",
    "        gen_opt = torch.optim.Adam(gen_p, 1e-4, (.5, .9))\n",
    "        dis_opt = torch.optim.Adam(dis_p, 1e-4, (.5, .9))\n",
    "\n",
    "        return gen_opt, dis_opt\n",
    "\n",
    "    def split_features(self, features):\n",
    "        feature_real = []\n",
    "        feature_fake = []\n",
    "        for scale in features:\n",
    "            true, fake = zip(*map(\n",
    "                lambda x: torch.split(x, x.shape[0] // 2, 0),\n",
    "                scale,\n",
    "            ))\n",
    "            feature_real.append(true)\n",
    "            feature_fake.append(fake)\n",
    "        return feature_real, feature_fake\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        batch = batch.to(\"cuda\")\n",
    "        p = Profiler()\n",
    "        gen_opt, dis_opt = self.optimizers()\n",
    "\n",
    "        with torch.autocast(device_type=\"cuda\", dtype=torch.float16, enabled=True):\n",
    "            x = batch.unsqueeze(1)\n",
    "\n",
    "            if self.pqmf is not None:\n",
    "                x_multiband = self.pqmf(x)\n",
    "            else:\n",
    "                x_multiband = x\n",
    "            p.tick('decompose')\n",
    "\n",
    "            self.encoder.set_warmed_up(self.warmed_up)\n",
    "            self.decoder.set_warmed_up(self.warmed_up)\n",
    "\n",
    "            # ENCODE INPUT\n",
    "            if self.enable_pqmf_encode:\n",
    "                z_pre_reg = self.encoder(x_multiband)\n",
    "            else:\n",
    "                z_pre_reg = self.encoder(x)\n",
    "\n",
    "            z, reg = self.encoder.reparametrize(z_pre_reg)[:2]\n",
    "            p.tick('encode')\n",
    "\n",
    "            # DECODE LATENT\n",
    "            y_multiband = self.decoder(z)\n",
    "            p.tick('decode')\n",
    "\n",
    "            if self.valid_signal_crop and self.receptive_field.sum():\n",
    "                x_multiband = rave.core.valid_signal_crop(\n",
    "                    x_multiband,\n",
    "                    *self.receptive_field,\n",
    "                )\n",
    "                y_multiband = rave.core.valid_signal_crop(\n",
    "                    y_multiband,\n",
    "                    *self.receptive_field,\n",
    "                )\n",
    "            p.tick('crop')\n",
    "\n",
    "            # DISTANCE BETWEEN INPUT AND OUTPUT\n",
    "            distances = {}\n",
    "\n",
    "            if self.pqmf is not None:\n",
    "                multiband_distance = self.multiband_audio_distance(\n",
    "                    x_multiband, y_multiband)\n",
    "                p.tick('mb distance')\n",
    "\n",
    "                x = self.pqmf.inverse(x_multiband)\n",
    "                y = self.pqmf.inverse(y_multiband)\n",
    "                p.tick('recompose')\n",
    "\n",
    "                for k, v in multiband_distance.items():\n",
    "                    distances[f'multiband_{k}'] = v\n",
    "            else:\n",
    "                x = x_multiband\n",
    "                y = y_multiband\n",
    "\n",
    "            fullband_distance = self.audio_distance(x, y)\n",
    "            p.tick('fb distance')\n",
    "\n",
    "            for k, v in fullband_distance.items():\n",
    "                distances[f'fullband_{k}'] = v\n",
    "\n",
    "            feature_matching_distance = 0.\n",
    "\n",
    "            if self.warmed_up:  # DISCRIMINATION\n",
    "                xy = torch.cat([x, y], 0)\n",
    "                features = self.discriminator(xy)\n",
    "\n",
    "                feature_real, feature_fake = self.split_features(features)\n",
    "\n",
    "                loss_dis = 0\n",
    "                loss_adv = 0\n",
    "\n",
    "                pred_real = 0\n",
    "                pred_fake = 0\n",
    "\n",
    "                for scale_real, scale_fake in zip(feature_real, feature_fake):\n",
    "                    current_feature_distance = sum(\n",
    "                        map(\n",
    "                            self.feature_matching_fun,\n",
    "                            scale_real[self.num_skipped_features:],\n",
    "                            scale_fake[self.num_skipped_features:],\n",
    "                        )) / len(scale_real[self.num_skipped_features:])\n",
    "\n",
    "                    feature_matching_distance = feature_matching_distance + current_feature_distance\n",
    "\n",
    "                    _dis, _adv = self.gan_loss(scale_real[-1], scale_fake[-1])\n",
    "\n",
    "                    pred_real = pred_real + scale_real[-1].mean()\n",
    "                    pred_fake = pred_fake + scale_fake[-1].mean()\n",
    "\n",
    "                    loss_dis = loss_dis + _dis\n",
    "                    loss_adv = loss_adv + _adv\n",
    "\n",
    "                feature_matching_distance = feature_matching_distance / len(\n",
    "                    feature_real)\n",
    "\n",
    "            else:\n",
    "                pred_real = torch.tensor(0., device=\"cuda\").to(x)\n",
    "                pred_fake = torch.tensor(0., device=\"cuda\").to(x)\n",
    "                loss_dis = torch.tensor(0., device=\"cuda\").to(x)\n",
    "                loss_adv = torch.tensor(0., device=\"cuda\").to(x)\n",
    "            p.tick('discrimination')\n",
    "\n",
    "            # COMPOSE GEN LOSS\n",
    "            loss_gen = {}\n",
    "            loss_gen.update(distances)\n",
    "            p.tick('update loss gen dict')\n",
    "\n",
    "            if reg.item():\n",
    "                loss_gen['regularization'] = reg\n",
    "\n",
    "            if self.warmed_up:\n",
    "                loss_gen['feature_matching'] = feature_matching_distance\n",
    "                loss_gen['adversarial'] = loss_adv\n",
    "\n",
    "        # OPTIMIZATION\n",
    "        if not (batch_idx %\n",
    "                self.update_discriminator_every) and self.warmed_up:\n",
    "            dis_opt.zero_grad(set_to_none=True)\n",
    "            p.tick('dis opt')\n",
    "        else:\n",
    "            gen_opt.zero_grad(set_to_none=True)\n",
    "            self.balancer.backward(loss_gen, y_multiband, self.log, p.tick)\n",
    "            gen_opt.step()\n",
    "\n",
    "        # LOGGING\n",
    "        if self.warmed_up:\n",
    "            self.log(\"loss_dis\", loss_dis)\n",
    "            self.log(\"pred_real\", pred_real.mean())\n",
    "            self.log(\"pred_fake\", pred_fake.mean())\n",
    "\n",
    "        self.log_dict(loss_gen)\n",
    "        p.tick('logging')\n",
    "\n",
    "    def encode(self, x):\n",
    "        if self.pqmf is not None and self.enable_pqmf_encode:\n",
    "            x = self.pqmf(x)\n",
    "        z, = self.encoder.reparametrize(self.encoder(x))[:1]\n",
    "        return z\n",
    "\n",
    "    def decode(self, z):\n",
    "        y = self.decoder(z)\n",
    "        if self.pqmf is not None and self.enable_pqmf_decode:\n",
    "            y = self.pqmf.inverse(y)\n",
    "        return y\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decode(self.encode(x))\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        batch = batch.to(\"cuda\")\n",
    "        x = batch.unsqueeze(1)\n",
    "\n",
    "        if self.pqmf is not None:\n",
    "            x_multiband = self.pqmf(x)\n",
    "\n",
    "        if self.enable_pqmf_encode:\n",
    "            z = self.encoder(x_multiband)\n",
    "        else:\n",
    "            z = self.encoder(x)\n",
    "\n",
    "        if isinstance(self.encoder, VariationalEncoder):\n",
    "            mean = torch.split(z, z.shape[1] // 2, 1)[0]\n",
    "        else:\n",
    "            mean = None\n",
    "\n",
    "        z = self.encoder.reparametrize(z)[0]\n",
    "        y = self.decoder(z)\n",
    "\n",
    "        if self.pqmf is not None:\n",
    "            x = self.pqmf.inverse(x_multiband)\n",
    "            y = self.pqmf.inverse(y)\n",
    "\n",
    "        distance = self.audio_distance(x, y)\n",
    "\n",
    "        full_distance = sum(distance.values())\n",
    "\n",
    "        if self.trainer is not None:\n",
    "            self.log('validation', full_distance)\n",
    "\n",
    "        return torch.cat([x, y], -1), mean\n",
    "\n",
    "    def validation_epoch_end(self, out):\n",
    "        if not self.receptive_field.sum():\n",
    "            print(\"Computing receptive field for this configuration...\")\n",
    "            lrf, rrf = rave.core.get_rave_receptive_field(self)\n",
    "            self.receptive_field[0] = lrf\n",
    "            self.receptive_field[1] = rrf\n",
    "            print(\n",
    "                f\"Receptive field: {1000*lrf/self.sr:.2f}ms <-- x --> {1000*rrf/self.sr:.2f}ms\"\n",
    "            )\n",
    "\n",
    "        if not len(out): return\n",
    "\n",
    "        audio, z = list(zip(*out))\n",
    "        audio = list(map(lambda x: x.cpu(), audio))\n",
    "\n",
    "        # LATENT SPACE ANALYSIS\n",
    "        if not self.warmed_up and isinstance(self.encoder, VariationalEncoder):\n",
    "            z = torch.cat(z, 0)\n",
    "            z = rearrange(z, \"b c t -> (b t) c\")\n",
    "\n",
    "            self.latent_mean.copy_(z.mean(0))\n",
    "            z = z - self.latent_mean\n",
    "\n",
    "            pca = PCA(z.shape[-1]).fit(z.cpu().numpy())\n",
    "\n",
    "            components = pca.components_\n",
    "            components = torch.from_numpy(components).to(\"cuda\").to(z)\n",
    "            self.latent_pca.copy_(components)\n",
    "\n",
    "            var = pca.explained_variance_ / np.sum(pca.explained_variance_)\n",
    "            var = np.cumsum(var)\n",
    "\n",
    "            self.fidelity.copy_(torch.from_numpy(var).to(\"cuda\").to(self.fidelity))\n",
    "\n",
    "            var_percent = [.8, .9, .95, .99]\n",
    "            for p in var_percent:\n",
    "                self.log(\n",
    "                    f\"fidelity_{p}\",\n",
    "                    np.argmax(var > p).astype(np.float32),\n",
    "                )\n",
    "\n",
    "        y = torch.cat(audio, 0)[:8].reshape(-1).numpy()\n",
    "\n",
    "        if self.integrator is not None:\n",
    "            y = self.integrator(y)\n",
    "\n",
    "        self.logger.experiment.add_audio(\"audio_val\", y, self.eval_number,\n",
    "                                         self.sr)\n",
    "        self.eval_number += 1\n",
    "\n",
    "    def on_fit_start(self):\n",
    "        tb = self.logger.experiment\n",
    "\n",
    "        config = gin.operative_config_str()\n",
    "        config = config.split('\\n')\n",
    "        config = ['```'] + config + ['```']\n",
    "        config = '\\n'.join(config)\n",
    "        tb.add_text(\"config\", config)\n",
    "\n",
    "        model = str(self)\n",
    "        model = model.split('\\n')\n",
    "        model = ['```'] + model + ['```']\n",
    "        model = '\\n'.join(model)\n",
    "        tb.add_text(\"model\", model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55752da7-ed48-4843-a923-7f03a2cdff41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import rave\n",
    "from rave.pqmf import *\n",
    "\n",
    "try:\n",
    "    from .__version__ import *\n",
    "except:\n",
    "    __version__ = None\n",
    "    __commit__ = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "947a3b7d-3f94-449c-964b-d2a298dd35f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import rave.blocks\n",
    "\n",
    "@gin.configurable\n",
    "def normalization(module: nn.Module, mode: str = \"identity\"):\n",
    "    if mode == \"identity\":\n",
    "        return module\n",
    "    elif mode == \"weight_norm\":\n",
    "        return weight_norm(module)\n",
    "    else:\n",
    "        raise Exception(f\"Normalization mode {mode} not supported\")\n",
    "\n",
    "\n",
    "class SampleNorm(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x / torch.norm(x, 2, 1, keepdim=True)\n",
    "\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, module, cumulative_delay=0):\n",
    "        super().__init__()\n",
    "        additional_delay = module.cumulative_delay\n",
    "        self.aligned = cc.AlignBranches(\n",
    "            module,\n",
    "            nn.Identity(),\n",
    "            delays=[additional_delay, 0],\n",
    "        )\n",
    "        self.cumulative_delay = additional_delay + cumulative_delay\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_net, x_res = self.aligned(x)\n",
    "        return x_net + x_res\n",
    "\n",
    "\n",
    "class ResidualLayer(nn.Module):\n",
    "    def __init__(self, dim, kernel_size, dilations, cumulative_delay=0):\n",
    "        super().__init__()\n",
    "        net = []\n",
    "        cd = 0\n",
    "        for d in dilations:\n",
    "            net.append(nn.LeakyReLU(0.2))\n",
    "            net.append(\n",
    "                normalization(\n",
    "                    cc.Conv1d(\n",
    "                        dim,\n",
    "                        dim,\n",
    "                        kernel_size,\n",
    "                        dilation=d,\n",
    "                        padding=cc.get_padding(kernel_size, dilation=d),\n",
    "                        cumulative_delay=cd,\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "            cd = net[-1].cumulative_delay\n",
    "        self.net = Residual(\n",
    "            cc.CachedSequential(*net),\n",
    "            cumulative_delay=cumulative_delay,\n",
    "        )\n",
    "        self.cumulative_delay = self.net.cumulative_delay\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class DilatedUnit(nn.Module):\n",
    "    def __init__(self, dim: int, kernel_size: int, dilation: int) -> None:\n",
    "        super().__init__()\n",
    "        net = [\n",
    "            nn.LeakyReLU(0.2),\n",
    "            normalization(\n",
    "                cc.Conv1d(\n",
    "                    dim,\n",
    "                    dim,\n",
    "                    kernel_size=kernel_size,\n",
    "                    dilation=dilation,\n",
    "                    padding=cc.get_padding(\n",
    "                        kernel_size,\n",
    "                        dilation=dilation,\n",
    "                    ),\n",
    "                )\n",
    "            ),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            normalization(cc.Conv1d(dim, dim, kernel_size=1)),\n",
    "        ]\n",
    "\n",
    "        self.net = cc.CachedSequential(*net)\n",
    "        self.cumulative_delay = net[1].cumulative_delay\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, dim, kernel_size, dilations_list, cumulative_delay=0) -> None:\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        cd = 0\n",
    "\n",
    "        for dilations in dilations_list:\n",
    "            layers.append(\n",
    "                ResidualLayer(\n",
    "                    dim,\n",
    "                    kernel_size,\n",
    "                    dilations,\n",
    "                    cumulative_delay=cd,\n",
    "                )\n",
    "            )\n",
    "            cd = layers[-1].cumulative_delay\n",
    "\n",
    "        self.net = cc.CachedSequential(\n",
    "            *layers,\n",
    "            cumulative_delay=cumulative_delay,\n",
    "        )\n",
    "        self.cumulative_delay = self.net.cumulative_delay\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "@gin.configurable\n",
    "class ResidualStack(nn.Module):\n",
    "    def __init__(self, dim, kernel_sizes, dilations_list, cumulative_delay=0) -> None:\n",
    "        super().__init__()\n",
    "        blocks = []\n",
    "        for k in kernel_sizes:\n",
    "            blocks.append(ResidualBlock(dim, k, dilations_list))\n",
    "        self.net = cc.AlignBranches(*blocks, cumulative_delay=cumulative_delay)\n",
    "        self.cumulative_delay = self.net.cumulative_delay\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        x = torch.stack(x, 0).to(\"cuda\").sum(0)\n",
    "        return x\n",
    "\n",
    "\n",
    "class UpsampleLayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, ratio, cumulative_delay=0):\n",
    "        super().__init__()\n",
    "        net = [nn.LeakyReLU(0.2)]\n",
    "        if ratio > 1:\n",
    "            net.append(\n",
    "                normalization(\n",
    "                    cc.ConvTranspose1d(\n",
    "                        in_dim, out_dim, 2 * ratio, stride=ratio, padding=ratio // 2\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            net.append(\n",
    "                normalization(cc.Conv1d(in_dim, out_dim, 3, padding=cc.get_padding(3)))\n",
    "            )\n",
    "\n",
    "        self.net = cc.CachedSequential(*net)\n",
    "        self.cumulative_delay = self.net.cumulative_delay + cumulative_delay * ratio\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "@gin.configurable\n",
    "class NoiseGenerator(nn.Module):\n",
    "    def __init__(self, in_size, data_size, ratios, noise_bands):\n",
    "        super().__init__()\n",
    "        net = []\n",
    "        channels = [in_size] * len(ratios) + [data_size * noise_bands]\n",
    "        cum_delay = 0\n",
    "        for i, r in enumerate(ratios):\n",
    "            net.append(\n",
    "                cc.Conv1d(\n",
    "                    channels[i],\n",
    "                    channels[i + 1],\n",
    "                    3,\n",
    "                    padding=cc.get_padding(3, r),\n",
    "                    stride=r,\n",
    "                    cumulative_delay=cum_delay,\n",
    "                )\n",
    "            )\n",
    "            cum_delay = net[-1].cumulative_delay\n",
    "            if i != len(ratios) - 1:\n",
    "                net.append(nn.LeakyReLU(0.2))\n",
    "\n",
    "        self.net = cc.CachedSequential(*net)\n",
    "        self.data_size = data_size\n",
    "        self.cumulative_delay = self.net.cumulative_delay * int(np.prod(ratios))\n",
    "\n",
    "        self.register_buffer(\n",
    "            \"target_size\", torch.tensor(np.prod(ratios), device=\"cuda\").long()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        amp = mod_sigmoid(self.net(x) - 5)\n",
    "        amp = amp.permute(0, 2, 1)\n",
    "        amp = amp.reshape(amp.shape[0], amp.shape[1], self.data_size, -1)\n",
    "\n",
    "        ir = amp_to_impulse_response(amp, self.target_size)\n",
    "        noise = torch.rand_like(ir, device=\"cuda\") * 2 - 1\n",
    "\n",
    "        noise = fft_convolve(noise, ir).permute(0, 2, 1, 3)\n",
    "        noise = noise.reshape(noise.shape[0], noise.shape[1], -1)\n",
    "        return noise\n",
    "\n",
    "\n",
    "def normalize_dilations(\n",
    "    dilations: Union[Sequence[int], Sequence[Sequence[int]]], ratios: Sequence[int]\n",
    "):\n",
    "    if isinstance(dilations[0], int):\n",
    "        dilations = [dilations for _ in ratios]\n",
    "    return dilations\n",
    "\n",
    "\n",
    "class EncoderV2(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_size: int,\n",
    "        capacity: int,\n",
    "        ratios: Sequence[int],\n",
    "        latent_size: int,\n",
    "        n_out: int,\n",
    "        kernel_size: int,\n",
    "        dilations: Sequence[int],\n",
    "        keep_dim: bool = False,\n",
    "        recurrent_layer: Optional[Callable[[], nn.Module]] = None,\n",
    "        spectrogram: Optional[Callable[[], Spectrogram]] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        dilations_list = normalize_dilations(dilations, ratios)\n",
    "\n",
    "        if spectrogram is not None:\n",
    "            self.spectrogram = spectrogram().to(\"cuda\")\n",
    "        else:\n",
    "            self.spectrogram = None\n",
    "\n",
    "        net = [\n",
    "            normalization(\n",
    "                cc.Conv1d(\n",
    "                    data_size,\n",
    "                    capacity,\n",
    "                    kernel_size=kernel_size * 2 + 1,\n",
    "                    padding=cc.get_padding(kernel_size * 2 + 1),\n",
    "                ).to(\"cuda\")\n",
    "            ).to(\"cuda\"),\n",
    "        ]\n",
    "\n",
    "        num_channels = capacity\n",
    "        for r, dilations in zip(ratios, dilations_list):\n",
    "            # ADD RESIDUAL DILATED UNITS\n",
    "            for d in dilations:\n",
    "                net.append(\n",
    "                    Residual(\n",
    "                        DilatedUnit(\n",
    "                            dim=num_channels,\n",
    "                            kernel_size=kernel_size,\n",
    "                            dilation=d,\n",
    "                        ).to(\"cuda\")\n",
    "                    ).to(\"cuda\")\n",
    "                )\n",
    "\n",
    "            # ADD DOWNSAMPLING UNIT\n",
    "            net.append(nn.LeakyReLU(0.2).to(\"cuda\"))\n",
    "\n",
    "            if keep_dim:\n",
    "                out_channels = num_channels * r\n",
    "            else:\n",
    "                out_channels = num_channels * 2\n",
    "            net.append(\n",
    "                normalization(\n",
    "                    cc.Conv1d(\n",
    "                        num_channels,\n",
    "                        out_channels,\n",
    "                        kernel_size=2 * r,\n",
    "                        stride=r,\n",
    "                        padding=cc.get_padding(2 * r, r),\n",
    "                    ).to(\"cuda\")\n",
    "                ).to(\"cuda\")\n",
    "            )\n",
    "\n",
    "            num_channels = out_channels\n",
    "\n",
    "        net.append(nn.LeakyReLU(0.2).to(\"cuda\"))\n",
    "        net.append(\n",
    "            normalization(\n",
    "                cc.Conv1d(\n",
    "                    num_channels,\n",
    "                    latent_size * n_out,\n",
    "                    kernel_size=kernel_size,\n",
    "                    padding=cc.get_padding(kernel_size),\n",
    "                ).to(\"cuda\")\n",
    "            ).to(\"cuda\")\n",
    "        )\n",
    "\n",
    "        if recurrent_layer is not None:\n",
    "            net.append(recurrent_layer(latent_size * n_out).to(\"cuda\"))\n",
    "\n",
    "        self.net = cc.CachedSequential(*net).to(\"cuda\")\n",
    "        self.to(\"cuda\")\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.spectrogram is not None:\n",
    "            x = self.spectrogram(x[:, 0])[..., :-1]\n",
    "            x = torch.log1p(x).to(\"cuda\")\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class GeneratorV2(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_size: int,\n",
    "        capacity: int,\n",
    "        ratios: Sequence[int],\n",
    "        latent_size: int,\n",
    "        kernel_size: int,\n",
    "        dilations: Sequence[int],\n",
    "        keep_dim: bool = False,\n",
    "        recurrent_layer: Optional[Callable[[], nn.Module]] = None,\n",
    "        amplitude_modulation: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        dilations_list = normalize_dilations(dilations, ratios)[::-1]\n",
    "        ratios = ratios[::-1]\n",
    "\n",
    "        if keep_dim:\n",
    "            num_channels = np.prod(ratios) * capacity\n",
    "        else:\n",
    "            num_channels = 2 ** len(ratios) * capacity\n",
    "\n",
    "        net = []\n",
    "\n",
    "        if recurrent_layer is not None:\n",
    "            net.append(recurrent_layer(latent_size))\n",
    "\n",
    "        net.append(\n",
    "            normalization(\n",
    "                cc.Conv1d(\n",
    "                    latent_size,\n",
    "                    num_channels,\n",
    "                    kernel_size=kernel_size,\n",
    "                    padding=cc.get_padding(kernel_size),\n",
    "                )\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        for r, dilations in zip(ratios, dilations_list):\n",
    "            # ADD UPSAMPLING UNIT\n",
    "            if keep_dim:\n",
    "                out_channels = num_channels // r\n",
    "            else:\n",
    "                out_channels = num_channels // 2\n",
    "            net.append(nn.LeakyReLU(0.2))\n",
    "            net.append(\n",
    "                normalization(\n",
    "                    cc.ConvTranspose1d(\n",
    "                        num_channels, out_channels, 2 * r, stride=r, padding=r // 2\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "\n",
    "            num_channels = out_channels\n",
    "\n",
    "            # ADD RESIDUAL DILATED UNITS\n",
    "            for d in dilations:\n",
    "                net.append(\n",
    "                    Residual(\n",
    "                        DilatedUnit(\n",
    "                            dim=num_channels,\n",
    "                            kernel_size=kernel_size,\n",
    "                            dilation=d,\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        net.append(nn.LeakyReLU(0.2))\n",
    "        net.append(\n",
    "            normalization(\n",
    "                cc.Conv1d(\n",
    "                    num_channels,\n",
    "                    data_size * 2 if amplitude_modulation else data_size,\n",
    "                    kernel_size=kernel_size * 2 + 1,\n",
    "                    padding=cc.get_padding(kernel_size * 2 + 1),\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.net = cc.CachedSequential(*net)\n",
    "        self.amplitude_modulation = amplitude_modulation\n",
    "        self.to(\"cuda\")\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.net(x)\n",
    "\n",
    "        if self.amplitude_modulation:\n",
    "            x, amplitude = x.split(x.shape[1] // 2, 1)\n",
    "            x = x * torch.sigmoid(amplitude)\n",
    "\n",
    "        return torch.tanh(x)\n",
    "\n",
    "    def set_warmed_up(self, state: bool):\n",
    "        pass\n",
    "\n",
    "\n",
    "class VariationalEncoder(nn.Module):\n",
    "    def __init__(self, encoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder().to(\"cuda\")\n",
    "        self.register_buffer(\"warmed_up\", torch.tensor(0, device=\"cuda\"))\n",
    "        self.to(\"cuda\")\n",
    "\n",
    "    def reparametrize(self, z):\n",
    "        mean, scale = z.chunk(2, 1)\n",
    "        std = nn.functional.softplus(scale) + 1e-4\n",
    "        var = std * std\n",
    "        logvar = torch.log(var)\n",
    "\n",
    "        z = torch.randn_like(mean, device=\"cuda\") * std + mean\n",
    "        kl = (mean * mean + var - logvar - 1).sum(1).mean()\n",
    "\n",
    "        return z, kl\n",
    "\n",
    "    def set_warmed_up(self, state: bool):\n",
    "        state = torch.tensor(int(state), device=self.warmed_up.device)\n",
    "        self.warmed_up = state\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        z = self.encoder(x)\n",
    "        if self.warmed_up:\n",
    "            z = z.detach()\n",
    "        return z\n",
    "\n",
    "\n",
    "\n",
    "def unit_norm_vector_to_angles(x: torch.Tensor) -> torch.Tensor:\n",
    "    norms = x.flip(1).pow(2)\n",
    "    norms[:, 1] += norms[:, 0]\n",
    "    norms = norms[:, 1:]\n",
    "    norms = norms.cumsum(1).flip(1).sqrt()\n",
    "    angles = torch.arccos(x[:, :-1] / norms)\n",
    "    angles[:, -1] = torch.where(\n",
    "        x[:, -1] >= 0,\n",
    "        angles[:, -1],\n",
    "        2 * np.pi - angles[:, -1],\n",
    "    )\n",
    "    angles[:, :-1] = angles[:, :-1] / np.pi\n",
    "    angles[:, -1] = angles[:, -1] / (2 * np.pi)\n",
    "    return 2 * (angles - 0.5)\n",
    "\n",
    "\n",
    "def angles_to_unit_norm_vector(angles: torch.Tensor) -> torch.Tensor:\n",
    "    angles = (angles / 2 + 0.5) % 1\n",
    "    angles[:, :-1] = angles[:, :-1] * np.pi\n",
    "    angles[:, -1] = angles[:, -1] * (2 * np.pi)\n",
    "    cos = angles.cos()\n",
    "    sin = angles.sin().cumprod(dim=1)\n",
    "    cos = torch.cat(\n",
    "        [\n",
    "            cos,\n",
    "            torch.ones(cos.shape[0], 1, cos.shape[-1], device=\"cuda\").type_as(cos),\n",
    "        ],\n",
    "        1,\n",
    "    )\n",
    "    sin = torch.cat(\n",
    "        [\n",
    "            torch.ones(sin.shape[0], 1, sin.shape[-1], device=\"cuda\").type_as(sin),\n",
    "            sin,\n",
    "        ],\n",
    "        1,\n",
    "    )\n",
    "    return cos * sin\n",
    "\n",
    "\n",
    "def wrap_around_value(x: torch.Tensor, value: float = 1) -> torch.Tensor:\n",
    "    return (x + value) % (2 * value) - value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73a85a46-0d70-49b2-be76-58cc03354f2e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import rave.core\n",
    "\n",
    "def mod_sigmoid(x):\n",
    "    return 2 * torch.sigmoid(x)**2.3 + 1e-7\n",
    "\n",
    "\n",
    "def random_angle(min_f=20, max_f=8000, sr=24000):\n",
    "    min_f = np.log(min_f)\n",
    "    max_f = np.log(max_f)\n",
    "    rand = np.exp(random() * (max_f - min_f) + min_f)\n",
    "    rand = 2 * np.pi * rand / sr\n",
    "    return rand\n",
    "\n",
    "\n",
    "def get_augmented_latent_size(latent_size: int, noise_augmentation: int):\n",
    "    return latent_size + noise_augmentation\n",
    "\n",
    "\n",
    "def pole_to_z_filter(omega, amplitude=.9):\n",
    "    z0 = amplitude * np.exp(1j * omega)\n",
    "    a = [1, -2 * np.real(z0), abs(z0)**2]\n",
    "    b = [abs(z0)**2, -2 * np.real(z0), 1]\n",
    "    return b, a\n",
    "\n",
    "\n",
    "def random_phase_mangle(x, min_f, max_f, amp, sr):\n",
    "    angle = random_angle(min_f, max_f, sr)\n",
    "    b, a = pole_to_z_filter(angle, amp)\n",
    "    return lfilter(b, a, x)\n",
    "\n",
    "\n",
    "def amp_to_impulse_response(amp, target_size):\n",
    "    \"\"\"\n",
    "    transforms frequecny amps to ir on the last dimension\n",
    "    \"\"\"\n",
    "    amp = torch.stack([amp, torch.zeros_like(amp, device=\"cuda\")], -1)\n",
    "    amp = torch.view_as_complex(amp)\n",
    "    amp = fft.irfft(amp)\n",
    "\n",
    "    filter_size = amp.shape[-1]\n",
    "\n",
    "    amp = torch.roll(amp, filter_size // 2, -1)\n",
    "    win = torch.hann_window(filter_size, dtype=amp.dtype, device=amp.device)\n",
    "\n",
    "    amp = amp * win\n",
    "\n",
    "    amp = nn.functional.pad(\n",
    "        amp,\n",
    "        (0, int(target_size) - int(filter_size)),\n",
    "    )\n",
    "    amp = torch.roll(amp, -filter_size // 2, -1)\n",
    "\n",
    "    return amp\n",
    "\n",
    "\n",
    "def fft_convolve(signal, kernel):\n",
    "    \"\"\"\n",
    "    convolves signal by kernel on the last dimension\n",
    "    \"\"\"\n",
    "    signal = nn.functional.pad(signal, (0, signal.shape[-1]))\n",
    "    kernel = nn.functional.pad(kernel, (kernel.shape[-1], 0))\n",
    "\n",
    "    output = fft.irfft(fft.rfft(signal) * fft.rfft(kernel))\n",
    "    output = output[..., output.shape[-1] // 2:]\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def search_for_run(run_path, mode=\"last\"):\n",
    "    if run_path is None: return None\n",
    "    if \".ckpt\" in run_path: return run_path\n",
    "    ckpts = map(str, Path(run_path).rglob(\"*.ckpt\"))\n",
    "    ckpts = filter(lambda e: mode in os.path.basename(str(e)), ckpts)\n",
    "    ckpts = sorted(ckpts)\n",
    "    if len(ckpts): return ckpts[-1]\n",
    "    else: return None\n",
    "\n",
    "\n",
    "def setup_gpu():\n",
    "    return gpu.getAvailable(maxMemory=.05)\n",
    "\n",
    "\n",
    "def get_beta_kl(step, warmup, min_beta, max_beta):\n",
    "    if step > warmup: return max_beta\n",
    "    t = step / warmup\n",
    "    min_beta_log = np.log(min_beta)\n",
    "    max_beta_log = np.log(max_beta)\n",
    "    beta_log = t * (max_beta_log - min_beta_log) + min_beta_log\n",
    "    return np.exp(beta_log)\n",
    "\n",
    "\n",
    "def get_beta_kl_cyclic(step, cycle_size, min_beta, max_beta):\n",
    "    return get_beta_kl(step % cycle_size, cycle_size // 2, min_beta, max_beta)\n",
    "\n",
    "\n",
    "def get_beta_kl_cyclic_annealed(step, cycle_size, warmup, min_beta, max_beta):\n",
    "    min_beta = get_beta_kl(step, warmup, min_beta, max_beta)\n",
    "    return get_beta_kl_cyclic(step, cycle_size, min_beta, max_beta)\n",
    "\n",
    "\n",
    "def n_fft_to_num_bands(n_fft: int) -> int:\n",
    "    return n_fft // 2 + 1\n",
    "\n",
    "\n",
    "def hinge_gan(score_real, score_fake):\n",
    "    loss_dis = torch.relu(1 - score_real) + torch.relu(1 + score_fake)\n",
    "    loss_dis = loss_dis.mean()\n",
    "    loss_gen = -score_fake.mean()\n",
    "    return loss_dis, loss_gen\n",
    "\n",
    "\n",
    "def ls_gan(score_real, score_fake):\n",
    "    loss_dis = (score_real - 1).pow(2) + score_fake.pow(2)\n",
    "    loss_dis = loss_dis.mean()\n",
    "    loss_gen = (score_fake - 1).pow(2).mean()\n",
    "    return loss_dis, loss_gen\n",
    "\n",
    "\n",
    "def nonsaturating_gan(score_real, score_fake):\n",
    "    score_real = torch.clamp(torch.sigmoid(score_real), 1e-7, 1 - 1e-7)\n",
    "    score_fake = torch.clamp(torch.sigmoid(score_fake), 1e-7, 1 - 1e-7)\n",
    "    loss_dis = -(torch.log(score_real) + torch.log(1 - score_fake)).mean()\n",
    "    loss_gen = -torch.log(score_fake).mean()\n",
    "    return loss_dis, loss_gen\n",
    "\n",
    "\n",
    "@torch.enable_grad()\n",
    "def get_rave_receptive_field(model: nn.Module):\n",
    "    N = 2**15\n",
    "    model.eval()\n",
    "    device = next(iter(model.parameters())).device\n",
    "\n",
    "    for module in model.modules():\n",
    "        if hasattr(module, 'gru_state') or hasattr(module, 'temporal'):\n",
    "            module.disable()\n",
    "\n",
    "    while True:\n",
    "        x = torch.randn(1, 1, N, requires_grad=True, device=device)\n",
    "\n",
    "        z = model.encode(x)\n",
    "        y = model.decode(z)\n",
    "\n",
    "        y[0, 0, N // 2].backward()\n",
    "        assert x.grad is not None, \"input has no grad\"\n",
    "\n",
    "        grad = x.grad.data.reshape(-1)\n",
    "        left_grad, right_grad = grad.chunk(2, 0)\n",
    "        large_enough = (left_grad[0] == 0) and right_grad[-1] == 0\n",
    "        if large_enough:\n",
    "            break\n",
    "        else:\n",
    "            N *= 2\n",
    "    left_receptive_field = len(left_grad[left_grad != 0])\n",
    "    right_receptive_field = len(right_grad[right_grad != 0])\n",
    "    model.zero_grad()\n",
    "\n",
    "    for module in model.modules():\n",
    "        if hasattr(module, 'gru_state') or hasattr(module, 'temporal'):\n",
    "            module.enable()\n",
    "    ratio = x.shape[-1] // z.shape[-1]\n",
    "    rate = model.sr / ratio\n",
    "    return left_receptive_field, right_receptive_field\n",
    "\n",
    "\n",
    "def valid_signal_crop(x, left_rf, right_rf):\n",
    "    dim = x.shape[1]\n",
    "    x = x[..., left_rf.item() // dim:]\n",
    "    if right_rf.item():\n",
    "        x = x[..., :-right_rf.item() // dim]\n",
    "    return x\n",
    "\n",
    "\n",
    "def relative_distance(\n",
    "    x: torch.Tensor,\n",
    "    y: torch.Tensor,\n",
    "    norm: Callable[[torch.Tensor], torch.Tensor],\n",
    ") -> torch.Tensor:\n",
    "    return norm(x - y) / norm(x)\n",
    "\n",
    "\n",
    "def mean_difference(target: torch.Tensor,\n",
    "                    value: torch.Tensor,\n",
    "                    norm: str = 'L1',\n",
    "                    relative: bool = False):\n",
    "    diff = target - value\n",
    "    if norm == 'L1':\n",
    "        diff = diff.abs().mean()\n",
    "        if relative:\n",
    "            diff = diff / target.abs().mean()\n",
    "        return diff\n",
    "    elif norm == 'L2':\n",
    "        diff = (diff * diff).mean()\n",
    "        if relative:\n",
    "            diff = diff / (target * target).mean()\n",
    "        return diff\n",
    "    else:\n",
    "        raise Exception(f'Norm must be either L1 or L2, got {norm}')\n",
    "\n",
    "\n",
    "class MelScale(nn.Module):\n",
    "\n",
    "    def __init__(self, sample_rate: int, n_fft: int, n_mels: int) -> None:\n",
    "        super().__init__()\n",
    "        mel = li.filters.mel(sr=sample_rate, n_fft=n_fft, n_mels=n_mels)\n",
    "        mel = torch.from_numpy(mel).float().to(\"cuda\")\n",
    "        self.register_buffer('mel', mel)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        mel = self.mel.type_as(x)\n",
    "        y = torch.einsum('bft,mf->bmt', x, mel)\n",
    "        return y\n",
    "\n",
    "\n",
    "class MultiScaleSTFT(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 scales: Sequence[int],\n",
    "                 sample_rate: int,\n",
    "                 magnitude: bool = True,\n",
    "                 normalized: bool = False,\n",
    "                 num_mels: Optional[int] = None) -> None:\n",
    "        super().__init__()\n",
    "        self.scales = scales\n",
    "        self.magnitude = magnitude\n",
    "        self.num_mels = num_mels\n",
    "\n",
    "        self.stfts = []\n",
    "        self.mel_scales = []\n",
    "        for scale in scales:\n",
    "            self.stfts.append(\n",
    "                torchaudio.transforms.Spectrogram(\n",
    "                    n_fft=scale,\n",
    "                    win_length=scale,\n",
    "                    hop_length=scale // 4,\n",
    "                    normalized=normalized,\n",
    "                    power=None,\n",
    "                ))\n",
    "            if num_mels is not None:\n",
    "                self.mel_scales.append(\n",
    "                    MelScale(\n",
    "                        sample_rate=sample_rate,\n",
    "                        n_fft=scale,\n",
    "                        n_mels=num_mels,\n",
    "                    ))\n",
    "            else:\n",
    "                self.mel_scales.append(None)\n",
    "\n",
    "        self.stfts = nn.ModuleList(self.stfts)\n",
    "        self.mel_scales = nn.ModuleList(self.mel_scales)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Sequence[torch.Tensor]:\n",
    "        x = rearrange(x, \"b c t -> (b c) t\")\n",
    "        stfts = []\n",
    "        for stft, mel in zip(self.stfts, self.mel_scales):\n",
    "            y = stft(x)\n",
    "            if mel is not None:\n",
    "                y = mel(y)\n",
    "            if self.magnitude:\n",
    "                y = y.abs()\n",
    "            else:\n",
    "                y = torch.stack([y.real, y.imag], -1)\n",
    "            stfts.append(y)\n",
    "\n",
    "        return stfts\n",
    "\n",
    "\n",
    "class AudioDistanceV1(nn.Module):\n",
    "\n",
    "    def __init__(self, multiscale_stft: Callable[[], nn.Module],\n",
    "                 log_epsilon: float) -> None:\n",
    "        super().__init__()\n",
    "        self.multiscale_stft = multiscale_stft()\n",
    "        self.log_epsilon = log_epsilon\n",
    "\n",
    "    def forward(self, x: torch.Tensor, y: torch.Tensor):\n",
    "        stfts_x = self.multiscale_stft(x)\n",
    "        stfts_y = self.multiscale_stft(y)\n",
    "        distance = 0.\n",
    "\n",
    "        for x, y in zip(stfts_x, stfts_y):\n",
    "            logx = torch.log(x + self.log_epsilon)\n",
    "            logy = torch.log(y + self.log_epsilon)\n",
    "\n",
    "            lin_distance = mean_difference(x, y, norm='L2', relative=True)\n",
    "            log_distance = mean_difference(logx, logy, norm='L1')\n",
    "\n",
    "            distance = distance + lin_distance + log_distance\n",
    "\n",
    "        return {'spectral_distance': distance}\n",
    "\n",
    "\n",
    "class WeightedInstantaneousSpectralDistance(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 multiscale_stft: Callable[[], MultiScaleSTFT],\n",
    "                 weighted: bool = False) -> None:\n",
    "        super().__init__()\n",
    "        self.multiscale_stft = multiscale_stft()\n",
    "        self.weighted = weighted\n",
    "\n",
    "    def phase_to_instantaneous_frequency(self,\n",
    "                                         x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.unwrap(x)\n",
    "        x = self.derivative(x)\n",
    "        return x\n",
    "\n",
    "    def derivative(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x[..., 1:] - x[..., :-1]\n",
    "\n",
    "    def unwrap(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.derivative(x)\n",
    "        x = (x + np.pi) % (2 * np.pi)\n",
    "        return (x - np.pi).cumsum(-1)\n",
    "\n",
    "    def forward(self, target: torch.Tensor, pred: torch.Tensor):\n",
    "        stfts_x = self.multiscale_stft(target)\n",
    "        stfts_y = self.multiscale_stft(pred)\n",
    "        spectral_distance = 0.\n",
    "        phase_distance = 0.\n",
    "\n",
    "        for x, y in zip(stfts_x, stfts_y):\n",
    "            assert x.shape[-1] == 2\n",
    "\n",
    "            x = torch.view_as_complex(x)\n",
    "            y = torch.view_as_complex(y)\n",
    "\n",
    "            # AMPLITUDE DISTANCE\n",
    "            x_abs = x.abs()\n",
    "            y_abs = y.abs()\n",
    "\n",
    "            logx = torch.log1p(x_abs)\n",
    "            logy = torch.log1p(y_abs)\n",
    "\n",
    "            lin_distance = mean_difference(x_abs,\n",
    "                                           y_abs,\n",
    "                                           norm='L2',\n",
    "                                           relative=True)\n",
    "            log_distance = mean_difference(logx, logy, norm='L1')\n",
    "\n",
    "            spectral_distance = spectral_distance + lin_distance + log_distance\n",
    "\n",
    "            # PHASE DISTANCE\n",
    "            x_if = self.phase_to_instantaneous_frequency(x.angle())\n",
    "            y_if = self.phase_to_instantaneous_frequency(y.angle())\n",
    "\n",
    "            if self.weighted:\n",
    "                mask = torch.clip(torch.log1p(x_abs[..., 2:]), 0, 1)\n",
    "                x_if = x_if * mask\n",
    "                y_if = y_if * mask\n",
    "\n",
    "            phase_distance = phase_distance + mean_difference(\n",
    "                x_if, y_if, norm='L2')\n",
    "\n",
    "        return {\n",
    "            'spectral_distance': spectral_distance,\n",
    "            'phase_distance': phase_distance\n",
    "        }\n",
    "\n",
    "\n",
    "class EncodecAudioDistance(nn.Module):\n",
    "\n",
    "    def __init__(self, scales: int,\n",
    "                 spectral_distance: Callable[[int], nn.Module]) -> None:\n",
    "        super().__init__()\n",
    "        self.waveform_distance = WaveformDistance(norm='L1')\n",
    "        self.spectral_distances = nn.ModuleList(\n",
    "            [spectral_distance(scale) for scale in scales])\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        waveform_distance = self.waveform_distance(x, y)\n",
    "        spectral_distance = 0\n",
    "        for dist in self.spectral_distances:\n",
    "            spectral_distance = spectral_distance + dist(x, y)\n",
    "\n",
    "        return {\n",
    "            'waveform_distance': waveform_distance,\n",
    "            'spectral_distance': spectral_distance\n",
    "        }\n",
    "\n",
    "\n",
    "class WaveformDistance(nn.Module):\n",
    "\n",
    "    def __init__(self, norm: str) -> None:\n",
    "        super().__init__()\n",
    "        self.norm = norm\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        return mean_difference(y, x, self.norm)\n",
    "\n",
    "\n",
    "class SpectralDistance(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_fft: int,\n",
    "        sampling_rate: int,\n",
    "        norm: Union[str, Sequence[str]],\n",
    "        power: Union[int, None],\n",
    "        normalized: bool,\n",
    "        mel: Optional[int] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if mel:\n",
    "            self.spec = torchaudio.transforms.MelSpectrogram(\n",
    "                sampling_rate,\n",
    "                n_fft,\n",
    "                hop_length=n_fft // 4,\n",
    "                n_mels=mel,\n",
    "                power=power,\n",
    "                normalized=normalized,\n",
    "                center=False,\n",
    "                pad_mode=None,\n",
    "            )\n",
    "        else:\n",
    "            self.spec = torchaudio.transforms.Spectrogram(\n",
    "                n_fft,\n",
    "                hop_length=n_fft // 4,\n",
    "                power=power,\n",
    "                normalized=normalized,\n",
    "                center=False,\n",
    "                pad_mode=None,\n",
    "            )\n",
    "\n",
    "        if isinstance(norm, str):\n",
    "            norm = (norm, )\n",
    "        self.norm = norm\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = self.spec(x)\n",
    "        y = self.spec(y)\n",
    "\n",
    "        distance = 0\n",
    "        for norm in self.norm:\n",
    "            distance = distance + mean_difference(y, x, norm)\n",
    "        return distance\n",
    "\n",
    "\n",
    "class ProgressLogger(object):\n",
    "\n",
    "    def __init__(self, name: str) -> None:\n",
    "        self.env = lmdb.open(\"status\")\n",
    "        self.name = name\n",
    "\n",
    "    def update(self, **new_state):\n",
    "        current_state = self.__call__()\n",
    "        with self.env.begin(write=True) as txn:\n",
    "            current_state.update(new_state)\n",
    "            current_state = json.dumps(current_state)\n",
    "            txn.put(self.name.encode(), current_state.encode())\n",
    "\n",
    "    def __call__(self):\n",
    "        with self.env.begin(write=True) as txn:\n",
    "            current_state = txn.get(self.name.encode())\n",
    "        if current_state is not None:\n",
    "            current_state = json.loads(current_state.decode())\n",
    "        else:\n",
    "            current_state = {}\n",
    "        return current_state\n",
    "\n",
    "\n",
    "class LoggerCallback(pl.Callback):\n",
    "\n",
    "    def __init__(self, logger: ProgressLogger) -> None:\n",
    "        super().__init__()\n",
    "        self.state = {'step': 0, 'warmed': False}\n",
    "        self.logger = logger\n",
    "\n",
    "    def on_train_batch_end(self, trainer, pl_module, outputs, batch,\n",
    "                           batch_idx) -> None:\n",
    "        self.state['step'] += 1\n",
    "        self.state['warmed'] = pl_module.warmed_up\n",
    "\n",
    "        if not self.state['step'] % 100:\n",
    "            self.logger.update(**self.state)\n",
    "\n",
    "    def state_dict(self):\n",
    "        return self.state.copy()\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        self.state.update(state_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aeed2817-15ee-4d45-81ad-4e4058dd5010",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import rave.dataset\n",
    "def get_derivator_integrator(sr: int):\n",
    "    alpha = 1 / (1 + 1 / sr * 2 * np.pi * 10)\n",
    "    derivator = ([.5, -.5], [1])\n",
    "    integrator = ([alpha**2, -alpha**2], [1, -2 * alpha, alpha**2])\n",
    "\n",
    "    return lambda x: lfilter(*derivator, x), lambda x: lfilter(*integrator, x)\n",
    "\n",
    "\n",
    "class AudioDataset(data.Dataset):\n",
    "\n",
    "    @property\n",
    "    def env(self) -> lmdb.Environment:\n",
    "        if self._env is None:\n",
    "            self._env = lmdb.open(self._db_path, lock=False)\n",
    "        return self._env\n",
    "\n",
    "    @property\n",
    "    def keys(self) -> Sequence[str]:\n",
    "        if self._keys is None:\n",
    "            with self.env.begin() as txn:\n",
    "                self._keys = list(txn.cursor().iternext(values=False))\n",
    "        return self._keys\n",
    "\n",
    "    def __init__(self,\n",
    "                 db_path: str,\n",
    "                 audio_key: str = 'waveform',\n",
    "                 transforms: Optional[transforms.Transform] = None) -> None:\n",
    "        super().__init__()\n",
    "        self._db_path = db_path\n",
    "        self._audio_key = audio_key\n",
    "        self._env = None\n",
    "        self._keys = None\n",
    "        self._transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        with self.env.begin() as txn:\n",
    "            ae = AudioExample.FromString(txn.get(self.keys[index]))\n",
    "\n",
    "        buffer = ae.buffers[self._audio_key]\n",
    "        assert buffer.precision == AudioExample.Precision.INT16\n",
    "\n",
    "        audio = np.frombuffer(buffer.data, dtype=np.int16)\n",
    "        audio = audio.astype(np.float32) / (2**15 - 1)\n",
    "\n",
    "        if self._transforms is not None:\n",
    "            audio = self._transforms(audio)\n",
    "\n",
    "        return audio\n",
    "\n",
    "\n",
    "class LazyAudioDataset(data.Dataset):\n",
    "\n",
    "    @property\n",
    "    def env(self) -> lmdb.Environment:\n",
    "        if self._env is None:\n",
    "            self._env = lmdb.open(self._db_path, lock=False)\n",
    "        return self._env\n",
    "\n",
    "    @property\n",
    "    def keys(self) -> Sequence[str]:\n",
    "        if self._keys is None:\n",
    "            with self.env.begin() as txn:\n",
    "                self._keys = list(txn.cursor().iternext(values=False))\n",
    "        return self._keys\n",
    "\n",
    "    def __init__(self,\n",
    "                 db_path: str,\n",
    "                 n_signal: int,\n",
    "                 sampling_rate: int,\n",
    "                 transforms: Optional[transforms.Transform] = None) -> None:\n",
    "        super().__init__()\n",
    "        self._db_path = db_path\n",
    "        self._env = None\n",
    "        self._keys = None\n",
    "        self._transforms = transforms\n",
    "        self._n_signal = n_signal\n",
    "        self._sampling_rate = sampling_rate\n",
    "\n",
    "        self.parse_dataset()\n",
    "\n",
    "    def parse_dataset(self):\n",
    "        items = []\n",
    "        for key in tqdm(self.keys, desc='Discovering dataset'):\n",
    "            with self.env.begin() as txn:\n",
    "                ae = AudioExample.FromString(txn.get(key))\n",
    "            length = float(ae.metadata['length'])\n",
    "            n_signal = int(math.floor(length * self._sampling_rate))\n",
    "            n_chunks = n_signal // self._n_signal\n",
    "            items.append(n_chunks)\n",
    "        items = np.asarray(items)\n",
    "        items = np.cumsum(items)\n",
    "        self.items = items\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.items[-1]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        audio_id = np.where(index < self.items)[0][0]\n",
    "        if audio_id:\n",
    "            index -= self.items[audio_id - 1]\n",
    "\n",
    "        key = self.keys[audio_id]\n",
    "\n",
    "        with self.env.begin() as txn:\n",
    "            ae = AudioExample.FromString(txn.get(key))\n",
    "\n",
    "        audio = extract_audio(\n",
    "            ae.metadata['path'],\n",
    "            self._n_signal,\n",
    "            self._sampling_rate,\n",
    "            index * self._n_signal,\n",
    "        )\n",
    "\n",
    "        if self._transforms is not None:\n",
    "            audio = self._transforms(audio)\n",
    "\n",
    "        return audio\n",
    "\n",
    "\n",
    "class HTTPAudioDataset(data.Dataset):\n",
    "\n",
    "    def __init__(self, db_path: str):\n",
    "        super().__init__()\n",
    "        self.db_path = db_path\n",
    "        logging.info(\"starting remote dataset session\")\n",
    "        self.length = int(requests.get(\"/\".join([db_path, \"len\"])).text)\n",
    "        logging.info(\"connection established !\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        example = requests.get(\"/\".join([\n",
    "            self.db_path,\n",
    "            \"get\",\n",
    "            f\"{index}\",\n",
    "        ])).text\n",
    "        example = AudioExampleWrapper(base64.b64decode(example)).get(\"audio\")\n",
    "        return example.copy()\n",
    "\n",
    "\n",
    "def normalize_signal(x: np.ndarray, max_gain_db: int = 30):\n",
    "    peak = np.max(abs(x))\n",
    "    if peak == 0: return x\n",
    "\n",
    "    log_peak = 20 * np.log10(peak)\n",
    "    log_gain = min(max_gain_db, -log_peak)\n",
    "    gain = 10**(log_gain / 20)\n",
    "\n",
    "    return x * gain\n",
    "\n",
    "\n",
    "def get_dataset(db_path,\n",
    "                sr,\n",
    "                n_signal,\n",
    "                derivative: bool = False,\n",
    "                normalize: bool = False):\n",
    "    if db_path[:4] == \"http\":\n",
    "        return HTTPAudioDataset(db_path=db_path)\n",
    "    with open(os.path.join(db_path, 'metadata.yaml'), 'r') as metadata:\n",
    "        metadata = yaml.safe_load(metadata)\n",
    "    lazy = metadata['lazy']\n",
    "\n",
    "    transform_list = [\n",
    "        lambda x: x.astype(np.float32),\n",
    "        transforms.RandomCrop(n_signal),\n",
    "        transforms.RandomApply(\n",
    "            lambda x: random_phase_mangle(x, 20, 2000, .99, sr),\n",
    "            p=.8,\n",
    "        ),\n",
    "        transforms.Dequantize(16),\n",
    "    ]\n",
    "\n",
    "    if normalize:\n",
    "        transform_list.append(normalize_signal)\n",
    "\n",
    "    if derivative:\n",
    "        transform_list.append(get_derivator_integrator(sr)[0])\n",
    "\n",
    "    transform_list.append(lambda x: x.astype(np.float32))\n",
    "\n",
    "    transform_list = transforms.Compose(transform_list)\n",
    "\n",
    "    if lazy:\n",
    "        return LazyAudioDataset(db_path, n_signal, sr, transform_list)\n",
    "    else:\n",
    "        return AudioDataset(\n",
    "            db_path,\n",
    "            transforms=transform_list,\n",
    "        )\n",
    "\n",
    "\n",
    "@gin.configurable\n",
    "def split_dataset(dataset, percent, max_residual: Optional[int] = None):\n",
    "    split1 = max((percent * len(dataset)) // 100, 1)\n",
    "    split2 = len(dataset) - split1\n",
    "    if max_residual is not None:\n",
    "        split2 = min(max_residual, split2)\n",
    "        split1 = len(dataset) - split2\n",
    "    split1, split2 = data.random_split(\n",
    "        dataset,\n",
    "        [split1, split2],\n",
    "        generator=torch.Generator().manual_seed(42),\n",
    "    )\n",
    "    return split1, split2\n",
    "\n",
    "\n",
    "def random_angle(min_f=20, max_f=8000, sr=24000):\n",
    "    min_f = np.log(min_f)\n",
    "    max_f = np.log(max_f)\n",
    "    rand = np.exp(random() * (max_f - min_f) + min_f)\n",
    "    rand = 2 * np.pi * rand / sr\n",
    "    return rand\n",
    "\n",
    "\n",
    "def pole_to_z_filter(omega, amplitude=.9):\n",
    "    z0 = amplitude * np.exp(1j * omega)\n",
    "    a = [1, -2 * np.real(z0), abs(z0)**2]\n",
    "    b = [abs(z0)**2, -2 * np.real(z0), 1]\n",
    "    return b, a\n",
    "\n",
    "\n",
    "def random_phase_mangle(x, min_f, max_f, amp, sr):\n",
    "    angle = random_angle(min_f, max_f, sr)\n",
    "    b, a = pole_to_z_filter(angle, amp)\n",
    "    return lfilter(b, a, x)\n",
    "\n",
    "\n",
    "def extract_audio(path: str, n_signal: int, sr: int,\n",
    "                  start_sample: int) -> Iterable[np.ndarray]:\n",
    "    start_sec = start_sample / sr\n",
    "    length = n_signal / sr + 0.1\n",
    "    process = subprocess.Popen(\n",
    "        [\n",
    "            'ffmpeg',\n",
    "            '-v',\n",
    "            'error',\n",
    "            '-ss',\n",
    "            str(start_sec),\n",
    "            '-i',\n",
    "            path,\n",
    "            '-ar',\n",
    "            str(sr),\n",
    "            '-ac',\n",
    "            '1',\n",
    "            '-t',\n",
    "            str(length),\n",
    "            '-f',\n",
    "            's16le',\n",
    "            '-',\n",
    "        ],\n",
    "        stdout=subprocess.PIPE,\n",
    "    )\n",
    "\n",
    "    chunk = process.communicate()[0]\n",
    "\n",
    "    chunk = np.frombuffer(chunk, dtype=np.int16).astype(np.float32) / 2**15\n",
    "    chunk = np.concatenate([chunk, np.zeros(n_signal)], -1)\n",
    "    return chunk[:n_signal]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "218c6a69-f7c5-490a-8256-d49471de2159",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch, time, gc\n",
    "\n",
    "# Timing utilities\n",
    "start_time = None\n",
    "\n",
    "def start_timer():\n",
    "    global start_time\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_max_memory_allocated()\n",
    "    torch.cuda.synchronize()\n",
    "    start_time = time.time()\n",
    "\n",
    "def end_timer_and_print(local_msg):\n",
    "    torch.cuda.synchronize()\n",
    "    end_time = time.time()\n",
    "    print(\"\\n\" + local_msg)\n",
    "    print(\"Total execution time = {:.3f} sec\".format(end_time - start_time))\n",
    "    print(\"Max memory used by tensors = {} bytes\".format(torch.cuda.max_memory_allocated()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a7a5eb0-8594-45aa-b11e-4fc51985662c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "NAME = \"compile\"\n",
    "CONFIG = [\"v2\"]\n",
    "DB_PATH = \"/home/ubuntu/preprocessed/\"\n",
    "MAX_STEPS = 1\n",
    "VAL_EVERY = 1\n",
    "N_SIGNAL = 131072\n",
    "BATCH = 8\n",
    "ckpt = None\n",
    "OVERRIDE = []\n",
    "WORKERS = 8\n",
    "GPU = None\n",
    "DERIVATIVE = False\n",
    "NORMALIZE = False\n",
    "PROGRESS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cce54374-c2ec-44cb-a267-f6aa0f9b3f11",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_gin_extension(config_name: str) -> str:\n",
    "    if config_name[-4:] != '.gin':\n",
    "        config_name += '.gin'\n",
    "    return config_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30484ce8-50c9-40ca-904b-e6ac6fa02d8a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def setup():\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    gin.parse_config_files_and_bindings(\n",
    "        map(add_gin_extension, CONFIG),\n",
    "        OVERRIDE,\n",
    "    )\n",
    "    model = RAVE()\n",
    "    # model = torch.compile(model, mode=\"reduce-overhead\"); torch._dynamo.config.verbose=True\n",
    "    if DERIVATIVE:\n",
    "        model.integrator = get_derivator_integrator(model.sr)[1]\n",
    "\n",
    "    dataset = get_dataset(\n",
    "        DB_PATH, model.sr, N_SIGNAL, derivative=DERIVATIVE, normalize=NORMALIZE\n",
    "    )\n",
    "    train, val = split_dataset(dataset, 98)\n",
    "    num_workers = WORKERS\n",
    "\n",
    "    if os.name == \"nt\" or sys.platform == \"darwin\":\n",
    "        num_workers = 0\n",
    "\n",
    "    train = DataLoader(\n",
    "        train, BATCH, True, drop_last=True, num_workers=num_workers\n",
    "    )\n",
    "    val = DataLoader(val, BATCH, False, num_workers=num_workers)\n",
    "\n",
    "    # CHECKPOINT CALLBACKS\n",
    "    validation_checkpoint = pl.callbacks.ModelCheckpoint(\n",
    "        monitor=\"validation\", filename=\"best\")\n",
    "    last_checkpoint = pl.callbacks.ModelCheckpoint(filename=\"last\")\n",
    "    val_check = {}\n",
    "    if len(train) >= VAL_EVERY:\n",
    "        val_check[\"val_check_interval\"] = VAL_EVERY\n",
    "    else:\n",
    "        nepoch = VAL_EVERY // len(train)\n",
    "        val_check[\"check_val_every_n_epoch\"] = nepoch\n",
    "    gin_hash = hashlib.md5(\n",
    "        gin.operative_config_str().encode()).hexdigest()[:10]\n",
    "    RUN_NAME = f'{NAME}_{gin_hash}'\n",
    "    os.makedirs(os.path.join(\"runs\", RUN_NAME), exist_ok=True)\n",
    "    if GPU == [-1]:\n",
    "        gpu = 0\n",
    "    else:\n",
    "        gpu = GPU or setup_gpu()\n",
    "    print('selected gpu:', gpu)\n",
    "    accelerator = None\n",
    "    devices = None\n",
    "    if GPU == [-1]:\n",
    "        pass\n",
    "    elif torch.cuda.is_available():\n",
    "        accelerator = \"cuda\"\n",
    "        devices = GPU or setup_gpu()\n",
    "    elif torch.backends.mps.is_available():\n",
    "        print(\n",
    "            \"Training on mac is not available yet. Use --gpu -1 to train on CPU (not recommended).\"\n",
    "        )\n",
    "        exit()\n",
    "        accelerator = \"mps\"\n",
    "        devices = 1\n",
    "    \n",
    "    trainer = pl.Trainer(\n",
    "        logger=pl.loggers.TensorBoardLogger(\n",
    "            \"runs\",\n",
    "            name=RUN_NAME,\n",
    "        ),\n",
    "        accelerator=accelerator,\n",
    "        devices=devices,\n",
    "        callbacks=[\n",
    "            validation_checkpoint,\n",
    "            last_checkpoint,\n",
    "            WarmupCallback(),\n",
    "            QuantizeCallback(),\n",
    "            LoggerCallback(ProgressLogger(RUN_NAME)),\n",
    "        ],\n",
    "        max_epochs=100000,\n",
    "        max_steps=MAX_STEPS,\n",
    "        profiler=\"simple\",\n",
    "        enable_progress_bar=PROGRESS,\n",
    "        **val_check,\n",
    "    )\n",
    "    run = search_for_run(ckpt)\n",
    "    if run is not None:\n",
    "        step = torch.load(run, map_location='cpu')[\"global_step\"]\n",
    "        trainer.fit_loop.epoch_loop._batches_that_stepped = step\n",
    "\n",
    "    with open(os.path.join(\"runs\", RUN_NAME, \"config.gin\"), \"w\") as config_out:\n",
    "        config_out.write(gin.operative_config_str())\n",
    "\n",
    "    return trainer, model, train, val, run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e5631b6-5262-4117-b607-d24e7f3106fc",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Path not found: v2.gin\n",
      "ERROR:root:Path not found: /home/ubuntu/rave-training/rave/v2.gin\n",
      "ERROR:root:Path not found: configs/v1.gin\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "RAVE.__init__() missing 13 required positional arguments: 'latent_size', 'sampling_rate', 'encoder', 'decoder', 'discriminator', 'phase_1_duration', 'gan_loss', 'valid_signal_crop', 'feature_matching_fun', 'num_skipped_features', 'audio_distance', 'multiband_audio_distance', and 'balancer'\n  No values supplied by Gin or caller for arguments: ['audio_distance', 'balancer', 'decoder', 'discriminator', 'encoder', 'feature_matching_fun', 'gan_loss', 'latent_size', 'multiband_audio_distance', 'num_skipped_features', 'phase_1_duration', 'sampling_rate', 'valid_signal_crop']\n  Gin had values bound for: []\n  Caller supplied values for: ['self']\n  In call to configurable 'RAVE' (<class '__main__.RAVE'>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer, model, train, val, run \u001b[38;5;241m=\u001b[39m \u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 7\u001b[0m, in \u001b[0;36msetup\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mcudnn\u001b[38;5;241m.\u001b[39mbenchmark \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      3\u001b[0m gin\u001b[38;5;241m.\u001b[39mparse_config_files_and_bindings(\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mmap\u001b[39m(add_gin_extension, CONFIG),\n\u001b[1;32m      5\u001b[0m     OVERRIDE,\n\u001b[1;32m      6\u001b[0m )\n\u001b[0;32m----> 7\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mRAVE\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# model = torch.compile(model, mode=\"reduce-overhead\"); torch._dynamo.config.verbose=True\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DERIVATIVE:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.11/lib/python3.10/site-packages/gin/config.py:1605\u001b[0m, in \u001b[0;36m_make_gin_wrapper.<locals>.gin_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1603\u001b[0m scope_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m in scope \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(scope_str) \u001b[38;5;28;01mif\u001b[39;00m scope_str \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1604\u001b[0m err_str \u001b[38;5;241m=\u001b[39m err_str\u001b[38;5;241m.\u001b[39mformat(name, fn_or_cls, scope_info)\n\u001b[0;32m-> 1605\u001b[0m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maugment_exception_message_and_reraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merr_str\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.11/lib/python3.10/site-packages/gin/utils.py:41\u001b[0m, in \u001b[0;36maugment_exception_message_and_reraise\u001b[0;34m(exception, message)\u001b[0m\n\u001b[1;32m     39\u001b[0m proxy \u001b[38;5;241m=\u001b[39m ExceptionProxy()\n\u001b[1;32m     40\u001b[0m ExceptionProxy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(exception)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\n\u001b[0;32m---> 41\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m proxy\u001b[38;5;241m.\u001b[39mwith_traceback(exception\u001b[38;5;241m.\u001b[39m__traceback__) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.11/lib/python3.10/site-packages/gin/config.py:1582\u001b[0m, in \u001b[0;36m_make_gin_wrapper.<locals>.gin_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1579\u001b[0m new_kwargs\u001b[38;5;241m.\u001b[39mupdate(kwargs)\n\u001b[1;32m   1581\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1582\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnew_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnew_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1583\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m   1584\u001b[0m   err_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: RAVE.__init__() missing 13 required positional arguments: 'latent_size', 'sampling_rate', 'encoder', 'decoder', 'discriminator', 'phase_1_duration', 'gan_loss', 'valid_signal_crop', 'feature_matching_fun', 'num_skipped_features', 'audio_distance', 'multiband_audio_distance', and 'balancer'\n  No values supplied by Gin or caller for arguments: ['audio_distance', 'balancer', 'decoder', 'discriminator', 'encoder', 'feature_matching_fun', 'gan_loss', 'latent_size', 'multiband_audio_distance', 'num_skipped_features', 'phase_1_duration', 'sampling_rate', 'valid_signal_crop']\n  Gin had values bound for: []\n  Caller supplied values for: ['self']\n  In call to configurable 'RAVE' (<class '__main__.RAVE'>)"
     ]
    }
   ],
   "source": [
    "trainer, model, train, val, run = setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8becc8a6-08cd-474b-b330-f5659eceb4fa",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.fit(model, train, val, ckpt_path=run)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rave_env",
   "language": "python",
   "name": "rave_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
